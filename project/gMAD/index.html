
<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <title>The gMAD Competition Methodology</title>

  <!-- CSS  -->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
  <div class="section no-pad-bot" id="index-banner">
    <div class="container scrollspy" id="home">

      <h4 class="header center black-text">Group Maximum Differentiation Competition: <br> Model Comparison with Few Samples</h4>

      <br>

      <div class="row center">
        <h5 class="header col s3">
          <div class="author"><a href="https://kedema.org" target="blank">Kede Ma</a></div>
          <div class="school"><a href="https://www.uwaterloo.ca/" target="blank">University of Waterloo</a></div>
          <div class="school"><a href="https://www.nyu.edu/" target="blank">New York University</a></div>
        </h5>
        <h5 class="header col s3">
          <div class="author"><a href="https://ece.uwaterloo.ca/~zduanmu/" target="blank">Zhengfang Duanmu</a></div>
          <div class="school"><a href="https://www.uwaterloo.ca/" target="blank">University of Waterloo</a></div>
        </h5>
            <h5 class="header col s3">
          <div class="author"><a href="https://ece.uwaterloo.ca/~z70wang/" target="blank">Zhou Wang</a></div>
          <div class="school"><a href="https://www.uwaterloo.ca/" target="blank">University of Waterloo</a></div>
        </h5>
        <h5 class="header col s3">
          <div class="author"><a href="http://ivipc.uestc.edu.cn/wqb/" target="blank">Qingbo Wu</a></div>
          <div class="school"><a href="http://en.uestc.edu.cn/" target="blank">UESTC</a></div>
          <div class="school"><a href="https://www.uwaterloo.ca/" target="blank">University of Waterloo</a></div>
        </h5>
      </div>

      <div class="row center">
		<h5 class="header col s3">
          <div class="author">Wentao Liu</div>
          <div class="school"><a href="https://www.uwaterloo.ca/" target="blank">University of Waterloo</a></div>
        </h5>

        <h5 class="header col s3">
          <div class="author">Hongwei Yong</div>
          <div class="school"><a href="http://en.xjtu.edu.cn/" target="blank">XJTU</a></div>
          <div class="school"><a href="https://www.polyu.edu.hk/" target="blank">HK PolyU</a></div>
        </h5>
        <h5 class="header col s3">
          <div class="author"><a href="http://ivipc.uestc.edu.cn/hlli/" target="blank">Hongliang Li</a></div>
          <div class="school"><a href="http://en.uestc.edu.cn/" target="blank">UESTC</a></div>
        </h5>
        <h5 class="header col s3">
          <div class="author"><a href="http://www4.comp.polyu.edu.hk/~cslzhang/" target="blank">Lei Zhang</a></div>
          <div class="school"><a href="https://www.polyu.edu.hk/" target="blank">HK PolyU</a></div>
        </h5>
        
      </div>

    </div>
  </div>


  <div class="container">

    <div class="section">

      <!--   Icon Section   -->
      <div class="row">
        <div class="col s12">
		 <a class="materialboxed"><img class="responsive-img" src="images/gMAD_ill.png"></a>
        </div>
      </div>

    </div>

    <br>

    <div class="row section scrollspy" id="abstract">
      <div class="title">Abstract</div>
      <p align="justify">In many science and engineering fields that require computational models to predict certain physical quantities, we are often faced with the selection of the best model under the constraint that only a small sample set can be physically measured. One such example is the prediction of human perception of visual quality, where sample images live in a high dimensional space with enormous content variations. We propose a new methodology for model comparison named group maximum differentiation (gMAD) competition. Given multiple computational models, gMAD maximizes the chances of falsifying a "defender" model using the rest models as "attackers". It exploits the sample space to find sample pairs that maximally differentiate the attackers while holding the defender fixed. Based on the results of the attacking-defending game, we introduce two measures, aggressiveness and resistance, to summarize the performance of each model at attacking other models and defending attacks from other models, respectively.  We demonstrate the gMAD competition using three examplesâ€”image quality, image aesthetics, and streaming video  quality-of-experience. Although these examples focus on visually discriminable quantities, the gMAD methodology can be extended to many other fields, and is especially useful when the sample space is large, the physical measurement is expensive and the cost of computational prediction is low.</p>
    </div>

    <!--<div class="row section scrollspy" id="paper">
      <div class="title">Paper</div>
        <div class="col s12">
          <img src="images/icon_pdf.png"> <a href="paper/cvpr16_gmad.pdf" target="_blank">CVPR 2016 paper</a>
        </div>
        <div class="col s12">
          <img src="images/icon_pdf.png"> <a href="paper/cvpr16_gmad_slide.pdf" target="_blank">CVPR 2016 spotlight slide</a>
        </div>
    </div>-->


    <div class="section row scrollspy" id="dataset">
      <div class="title">Downloads</div>
      <div class="row">
        <div class="col s3 center">
          <a href="../../paper/19_TPAMI_gMAD.pdf" target="_blank">
            <!--<i class="fa fa-folder-open fa-5x" aria-hidden="true"></i>  -->
            <img src="images/PDF.png">
          </a>
          <br>
          <a href="../../paper/19_TPAMI_gMAD.pdf" target="_blank">PDF</a>
        </div>
        <div class="col s3 center">
          <a href="../../paper/16_CVPR_gMAD.pdf" target="_blank">
            <!--<i class="fa fa-folder-open fa-5x" aria-hidden="true"></i>  -->
            <img src="images/PDF.png">
          </a>
          <br>
          <a href="../../paper/16_CVPR_gMAD.pdf" target="_blank">CVPR version</a>
        </div>
		
		<div class="col s3 center">
          <a href="../../data/waterloo_exploration.rar" target="_blank">
            <img src="images/ZIP.png">
          </a>
          <br>
          <a href="http://ivc.uwaterloo.ca/database/WaterlooExploration/waterloo_exploration.rar" target="_blank">Waterloo Exploration Database</a>
        </div>
        <div class="col s3 center">
          <a href="../../code/gMADToolboxV1.0-beta3.rar" target="_blank">
            <img src="images/ZIP.png">
          </a>
          <br>
          <a href="../../code/gMADToolboxV1.0-beta3.rar" target="_blank">Toolbox for IQA</a>
        </div>
       </div>


      <div class="row">
        <p align="justify">Instruction:</p> 
        <ol>
          <li align="justify">The gMAD toolbox for IQA has a dependency upon the LIVE Image Quality Assessment database, which can be downloaded from <a href="http://live.ece.utexas.edu/research/quality/subjective.htm">Here</a>.</li>
          <li align="justify">Download the Waterloo Exploration Database and the gMAD toolbox.</li>
          <li align="justify">Put your algorithm in the root folder of the toolbox, and execute the initialization script.</li>
          <li align="justify">After the initialization step, you can find all generated image pairs in the ./data/test_image folder, from which you may gain a first impression on how the models compete with each other.</li>
          <li align="justify">Execute run_subjective_test script to start subjective testing.</li>
          <li align="justify">Go to the ./support_functions/cvx folder and execute the cvx_setup script. Then execute data_analysis script to obtain global ranking results.</li>
        </ol> 
      </div>
     </div>
	  
	  <div class="section row scrollspy" id="bib">
      <div class="title">Bibtex</div>
              <pre>@article{ma2020group,
  author    = {Ma, Kede and Duanmu, Zhengfang and Wang, Zhou and Wu, Qingbo and Liu, Wentao and Yong, Hongwei and Li, Hongliang and Zhang, Lei}, 
  title     = {Group Maximum Differentiation Competition: Model Comparison with Few Samples}, 
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {42},
  number    = {4},
  pages     = {851-864},
  month     = {Apr.},
  year      = {2020}
  }
              </pre>
        <pre>@inproceeding{ma2016gmad,
  author    = {Ma, Kede and Wu, Qingbo and Wang, Zhou and Duanmu, Zhengfang and Yong, Hongwei and Li, Hongliang and Zhang, Lei}, 
  title     = {Group {MAD} Competition - A New Methodology to Compare Objective Image Quality Models}, 
  booktitle = {IEEE Conferene on Computer Vision and Pattern Recognition},
  pages     = {1664--1673},
  year      = {2016}}
            </pre>
    </div>


    <div class="section row scrollspy" id="results">
      <div class="title">Results</div>
      <br>
      <div class="subtitle">Image Quality Assessment</div>
      <table class="bordered striped">
        <thead>
          <tr>
              <th data-field="id">Algorithm</th>
              <th data-field="name">Reference</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>PSNR</td>
            <td>Peak signal-to-noise ratio</td>
          </tr>
          <tr>
            <td>SSIM</td>
            <td>Wang et al. Image quality assessment: from error visibility to structural similarity. TIP. 2004.</td>
          </tr>
          <tr>
            <td>MS-SSIM</td>
            <td>Wang et al. Multi-scale structural similarity for image quality assessment. Asilomar. 2003.</td>
          </tr>
		  <tr>
            <td>FSIM</td>
            <td>Zhang et al. A feature similarity index for image quality assessment. TIP. 2011.</td>
          </tr>

          <tr>
            <td>BIQI</td>
            <td>Moorthy et al. A two-step framework for constructing blind image quality indices. SPL. 2010.</td>
          </tr>
          <tr>
            <td>BLINDS-II</td>
            <td>Saad et al. Blind image quality assessment: a natural scene statistics approach in the DCT domain. TIP. 2012.</td>
          </tr>
          <tr>
            <td>BRISQUE</td>
            <td>Mittal et al. No-reference image quality assessment in the spatial domain. TIP. 2012.</td>
          </tr>
          <tr>
            <td>CORNIA</td>
            <td>Ye et al. Unsupervised feature learning framework for no-reference image quality assessment. CVPR. 2012.</td>
          </tr>
          <tr>
            <td>DIIVINE</td>
            <td>Moorthy et al. Blind image quality assessment: from scene statistics to perceptual quality. TIP. 2011.</td>
          </tr>
          <tr>
            <td>IL-NIQE</td>
            <td>Zhang et al. A feature-enriched completely blind image quality evaluator. TIP. 2015.</td>
          </tr>
          <tr>
            <td>LPSI</td>
            <td>Wu et al. A highly efficient method for blind image quality assessment. ICIP. 2015.</td>
          </tr>
          <tr>
            <td>M3</td>
            <td>Xue et al. Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features. TIP. 2014.</td>
          </tr>
          <tr>
            <td>NFERM</td>
            <td>Gu et al. Using free energy principle for blind image quality assessment. TMM. 2015.</td>
          </tr>
          <tr>
            <td>NIQE</td>
            <td>Mittal et al. Making a completely blind image quality analyzer. SPL. 2013.</td>
          </tr>
          <tr>
            <td>QAC</td>
            <td>Xue et al. Learning without human scores for blind image quality assessment. CVPR. 2013.</td>
          </tr>
          <tr>
            <td>TCLT</td>
            <td>Wu et al. Blind image quality assessment based on multichannel features fusion and label transfer. TCSVT. 2016.</td>
          </tr>
        </tbody>
      </table>
      <br>

      <div class="subtitle">- Performance Comparison</div>
      <br>
      <div class="row">
        <div class="col s6">
          <a class="materialboxed"><img class="responsive-img" src="images/am.bmp"></a>
          <p align="center">Aggressiveness Matrix</p>
        </div>
        <div class="col s6">
          <a class="materialboxed"><img class="responsive-img" src="images/rm.bmp"></a>
          <p align="center">Resistance Matrix</p>
        </div>
      </div>
      
      <div class="row">
        <div class="col s12">
          <a class="materialboxed"><img class="responsive-img" src="images/gr.bmp"></a>
        </div>
      </div>
    </div>
    <br>
      <div class="subtitle">Image Aesthetics Assessment</div>
      <table class="bordered striped">
        <thead>
          <tr>
              <th data-field="id">Algorithm</th>
              <th data-field="name">Reference</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>GIST+SVR</td>
            <td>Oliva and Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. IJCV. 2001</td>
          </tr>
          <tr>
            <td>AAF+SVR</td>
            <td>Mavridaki and Mezaris.  A comprehensive aesthetic quality assessment method for natural images using basic rules of photography. ICIP. 2015.</td>
          </tr>
          <tr>
            <td>Kong16</td>
            <td>Kong et al. Photo aesthetics ranking network with attributes and content adaptation. ECCV. 2016.</td>
          </tr>
      <tr>
            <td>Jin16</td>
            <td>Jin et al. Image aesthetic predictors based on weighted CNNs. ICIP. 2016.</td>
          </tr>
        </tbody>
      </table>
      <br>
      <div class="subtitle">- Performance Comparison</div>
      <br>
      <div class="row center">
        <div class="col s12">
          <p align="center">Pairwise Results</p>
          <a class="materialboxed"><img class="responsive-img" src="images/iaa_pairwise.png"></a>
        </div>
      </div>
      <br>
           <div class="row center">
        <div class="col s12">
          <p align="center">Global Ranking Results</p>
          <a class="materialboxed"><img class="responsive-img" src="images/iaa_global.png"></a>
        </div>
      </div>
      <br>
      <div class="subtitle">Streaming Video QoE Assessment</div>
      <table class="bordered striped">
        <thead>
          <tr>
              <th data-field="id">Algorithm</th>
              <th data-field="name">Reference</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Liu12</td>
            <td>Liu et al. A case for a coordinated Internet video control plane. SIGCOMM. 2012</td>
          </tr>
          <tr>
            <td>Yin15</td>
            <td>Yin et al. A control-theoretic approach for dynamic adaptive video streaming over HTTP. SIGCOMM. 2015.</td>
          </tr>
          <tr>
            <td>SQI</td>
            <td>Duanmu et al. A quality-of-experience index for streaming video. JSTSP. 2017.</td>
          </tr>
        </tbody>
      </table>
      <br>
      <div class="subtitle">- Performance Comparison</div>
      <br>
      <div class="row center">
        <div class="col s12">
          <p align="center">Pairwise Results</p>
          <a class="materialboxed"><img class="responsive-img" src="images/qoe_pairwise.png"></a>
        </div>
      </div>
      <br>
           <div class="row center">
        <div class="col s12">
          <p align="center">Global Ranking Results</p>
          <a class="materialboxed"><img class="responsive-img" src="images/qoe_global.png"></a>
        </div>
      </div>
  </div>




	<div class="row center">
	<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=56cfnbg8c95&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
	</div>
  <footer class="page-footer white">
    <div class="footer-copyright center black-text">
      Copyright &#169; 2018
    </div>
  </footer>


  <!--  Scripts-->
  <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="js/materialize.js"></script>
  <script src="js/init.js"></script>

  </body>
</html>